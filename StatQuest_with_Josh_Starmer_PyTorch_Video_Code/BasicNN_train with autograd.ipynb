{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78baa6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #create tensors to raw data and weights\n",
    "import torch.nn as nn # make tensores part of nn\n",
    "import torch.nn.functional as F #gives activation function  \n",
    "from torch.optim import SGD #stochastic gradient descend\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32c087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNN_train(nn.Module):\n",
    "    def __init__(self):#creates and inicializes weights and biases + all objects will have it\n",
    "        super(BasicNN_train, self).__init__()\n",
    "        self.w00 = nn.Parameter(torch.tensor(1.7), requires_grad=False)\n",
    "        self.b00 = nn.Parameter(torch.tensor(-0.85), requires_grad=False)\n",
    "        self.w01 = nn.Parameter(torch.tensor(-40.8), requires_grad=False)\n",
    "\n",
    "        self.w10 = nn.Parameter(torch.tensor(12.6), requires_grad=False)\n",
    "        self.b10 = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        self.w11 = nn.Parameter(torch.tensor(2.7), requires_grad=False)\n",
    "\n",
    "        self.final_bias = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "        \n",
    "    def forward(self, input):#Makes foward pass trought neural network to see what input will give out\n",
    "        input_to_top_relu = (input*self.w00)+self.b00\n",
    "        top_relu_output = F.relu(input_to_top_relu)\n",
    "        scaled_top_relu_output = top_relu_output*self.w01\n",
    "        \n",
    "        input_to_bottom_relu = (input*self.w10)+self.b10\n",
    "        bottom_relu_output = F.relu(input_to_bottom_relu)\n",
    "        scaled_bottom_relu_output = bottom_relu_output*self.w11 \n",
    "\n",
    "        input_to_final_relu = scaled_top_relu_output + scaled_bottom_relu_output + self.final_bias\n",
    "        output = F.relu(input_to_final_relu)\n",
    "\n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "654858cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([0., 0.5, 1.])\n",
    "labels = torch.tensor([0., 1.  , 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1c784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicNN_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9facf46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel.parameters() - will optimize every parameters that has requires_grad=True\\nlr = learning rate\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "'''\n",
    "model.parameters() - will optimize every parameters that has requires_grad=True\n",
    "lr = learning rate\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e6b2f",
   "metadata": {},
   "source": [
    "Below, we create the for loop that does gradient despend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05541e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Final Bias: tensor(-3.2020)\n",
      "\n",
      "Step: 1 Final Bias: tensor(-5.7636)\n",
      "\n",
      "Step: 2 Final Bias: tensor(-7.8129)\n",
      "\n",
      "Step: 3 Final Bias: tensor(-9.4523)\n",
      "\n",
      "Step: 4 Final Bias: tensor(-10.7638)\n",
      "\n",
      "Step: 5 Final Bias: tensor(-11.8131)\n",
      "\n",
      "Step: 6 Final Bias: tensor(-12.6525)\n",
      "\n",
      "Step: 7 Final Bias: tensor(-13.3240)\n",
      "\n",
      "Step: 8 Final Bias: tensor(-13.8612)\n",
      "\n",
      "Step: 9 Final Bias: tensor(-14.2909)\n",
      "\n",
      "Step: 10 Final Bias: tensor(-14.6348)\n",
      "\n",
      "Step: 11 Final Bias: tensor(-14.9098)\n",
      "\n",
      "Step: 12 Final Bias: tensor(-15.1298)\n",
      "\n",
      "Step: 13 Final Bias: tensor(-15.3059)\n",
      "\n",
      "Step: 14 Final Bias: tensor(-15.4467)\n",
      "\n",
      "Step: 15 Final Bias: tensor(-15.5594)\n",
      "\n",
      "Step: 16 Final Bias: tensor(-15.6495)\n",
      "\n",
      "Step: 17 Final Bias: tensor(-15.7216)\n",
      "\n",
      "Step: 18 Final Bias: tensor(-15.7793)\n",
      "\n",
      "Step: 19 Final Bias: tensor(-15.8254)\n",
      "\n",
      "Step: 20 Final Bias: tensor(-15.8623)\n",
      "\n",
      "Step: 21 Final Bias: tensor(-15.8919)\n",
      "\n",
      "Step: 22 Final Bias: tensor(-15.9155)\n",
      "\n",
      "Step: 23 Final Bias: tensor(-15.9344)\n",
      "\n",
      "Step: 24 Final Bias: tensor(-15.9495)\n",
      "\n",
      "Step: 25 Final Bias: tensor(-15.9616)\n",
      "\n",
      "Step: 26 Final Bias: tensor(-15.9713)\n",
      "\n",
      "Step: 27 Final Bias: tensor(-15.9790)\n",
      "\n",
      "Step: 28 Final Bias: tensor(-15.9852)\n",
      "\n",
      "Step: 29 Final Bias: tensor(-15.9902)\n",
      "\n",
      "Step: 30 Final Bias: tensor(-15.9941)\n",
      "\n",
      "Step: 31 Final Bias: tensor(-15.9973)\n",
      "\n",
      "Step: 32 Final Bias: tensor(-15.9999)\n",
      "\n",
      "Step: 33 Final Bias: tensor(-16.0019)\n",
      "\n",
      "Num stepts: 34\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for iteration in range(len(inputs)):\n",
    "        input_i = inputs[iteration]\n",
    "        labels_i = labels[iteration]\n",
    "        output_i = model(input_i)\n",
    "        \n",
    "        loss = (output_i-labels_i)**2#this is the square residual loss function. there are others losso functions\n",
    "        \n",
    "        loss.backward()#calculate de derivative of the loss function with the respect of the parameters I want to optiize\n",
    "        #loss.backwars adds derivatives  - accumulates the derivatives\n",
    "        total_loss += float(loss)\n",
    "        \n",
    "    if (total_loss < 0.0001):\n",
    "        print(\"Num stepts: \" + str(epoch))\n",
    "        break\n",
    "\n",
    "    optimizer.step()#is total_loss isnÂ´t small, we take a small step towars a better value for the final bias\n",
    "    #optimizer.step() has acess to derivatives stored in model. This allows to change weights in a correct way\n",
    "    optimizer.zero_grad()#all derivatives in model become 0\n",
    "\n",
    "    print(\"Step: \" + str(epoch) + \" Final Bias: \" + str(model.final_bias.data) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe85ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
